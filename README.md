# Inroduction of ALBERT to understand Korean language in Financial domain with Transformers

This tutorial presented at PyCon Korea 2020 was created to share presentation slides and example codes.
Since this tutorial mostly focused on concept of language model and its usages with Hugging Face's Transformers, 
please refer the [link](https://github.com/KB-Bank-AI/KB-ALBERT-KO) if you are interested in pre-trained KB-ALBERT model and its details.
I appreciate Hugging Face Teams and their contributions to open source communities. 

íŒŒì´ì½˜ ì½”ë¦¬ì•„ 2020ì—ì„œ ë°œí‘œí•œ "ê¸ˆìœµ ì–¸ì–´ ì´í•´ë¥¼ ìœ„í•´ ê°œë°œëœ ALBERT í†ºì•„ë³´ê¸° with Transformers"ì˜ 
ë°œí‘œìë£Œ ë° ì˜ˆì œ ê³µìœ ë¥¼ ìœ„í•œ ê¹ƒí—™ í˜ì´ì§€ì…ë‹ˆë‹¤. ë³¸ ë°œí‘œëŠ” *ì–¸ì–´ëª¨ë¸ì˜ ì»¨ì…‰*ê³¼ *Hugging Faceì˜ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ë°©ë²•*ì„ 
ì¤‘ì ìœ¼ë¡œ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤. KB-ALBERT ëª¨ë¸ ìì²´ ì •ë³´ì— ëŒ€í•´ì„œëŠ” [link](https://github.com/KB-Bank-AI/KB-ALBERT-KO)ì—ì„œ 
ì°¾ì•„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br>

## Slides

Please find Korean slides [here](https://github.com/sackoh/pycon-korea-2020-kb-albert/blob/master/assets/PyConKR-2020_%EC%98%A4%EC%84%B1%EC%9A%B0_%EA%B8%88%EC%9C%B5%20%EC%96%B8%EC%96%B4%20%EC%9D%B4%ED%95%B4%EB%A5%BC%20%EC%9C%84%ED%95%B4%20%EA%B0%9C%EB%B0%9C%EB%90%9C%20ALBERT%20%ED%86%BA%EC%95%84%EB%B3%B4%EA%B8%B0_v1.0.pdf), but english version is being prepared.

ë°œí‘œì— ì‚¬ìš©ëœ PPT ìë£Œ [link](https://github.com/sackoh/pycon-korea-2020-kb-albert/blob/master/assets/PyConKR-2020_%EC%98%A4%EC%84%B1%EC%9A%B0_%EA%B8%88%EC%9C%B5%20%EC%96%B8%EC%96%B4%20%EC%9D%B4%ED%95%B4%EB%A5%BC%20%EC%9C%84%ED%95%B4%20%EA%B0%9C%EB%B0%9C%EB%90%9C%20ALBERT%20%ED%86%BA%EC%95%84%EB%B3%B4%EA%B8%B0_v1.0.pdf) ì…ë‹ˆë‹¤.

## Video Recording

A recording of the tutorial would be published on YouTube.

ë°œí‘œ ë…¹í™” ì´¬ì˜ ë¶„ì€ PyCon YouTubeì— ê²Œì‹œë  ì˜ˆì •ì…ë‹ˆë‹¤.

<br>

## Abstract
In Natural Language Processing (NLP), pretraining and fine-tuning have become usual in many tasks. 
Pretraining language models (PLMs) (e.g., BERT, XLNet, RoBERTa, ALBERT, ELECTRA) contain many layers and parameters 
and require large corpus and hardware resources.
So, it is hard for individual users to pretrain LMs with limited resources. 
Google, Facebook and other contributors have shared their works and pretrained models. 
Following these open source trends, In Korea, ETRI, SKT have also opened PLMs (e.g. KorBERT, KoBERT) for both individual 
and organizational users.
However, there are still some restrictions for practitioners to fine-tune at other domains.
This tutorial might be helpful to ones who want to fine-tune and deploy PLMs easily for their domains.

ìµœê·¼ ìì—°ì–´ì²˜ë¦¬ ë¶„ì•¼ëŠ” ì‚¬ì „í•™ìŠµí•œ ì–¸ì–´ëª¨ë¸ë“¤(PLMs) (e.g. BERT, XLNet, ALBERT, ELECTRA)ì„ í™œìš©í•œ ë¯¸ì„¸ì¡°ì • ë“±ì„ í•˜ëŠ” ê²ƒì´ 
ì¼ë°˜ì ì´ê²Œ ëìŠµë‹ˆë‹¤. êµ¬ê¸€, í˜ì´ìŠ¤ë¶ì„ ë¹„ë¡¯í•˜ì—¬ êµ­ë‚´ì—ì„œëŠ” ETRI, SKTì—ì„œ KorBERT, KoBERTë¥¼ ê³µê°œí–ˆìŠµë‹ˆë‹¤.
í•˜ì§€ë§Œ ì‹¤ì œ ì´ë¥¼ ìì‹ ì˜ ë„ë©”ì¸ì—ì„œ í™œìš©í•˜ê¸°ë€ ì‰½ì§€ ì•ŠìŠµë‹ˆë‹¤. ìµœê·¼ ê°€ì¥ ì¸ê¸°ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¤‘ í•˜ë‚˜ì¸ ğŸ¤— Transformersë¥¼ 
í†µí•´ ìì‹ ì˜ ëª©ì ì— ë§ê²Œ ì´ˆë³´ìë¶€í„° ì „ë¬¸ì ì¸ ë”¥ëŸ¬ë‹ ì—°êµ¬ìê¹Œì§€ ëˆ„êµ¬ë‚˜ ì‰½ê²Œ ë¸”ë¡ í˜•íƒœë¡œ transformer ì•„í‚¤í…ì²˜ë¥¼ ë¶ˆëŸ¬ì™€ 
ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë²ˆ ë°œí‘œê°€ ìì‹ ì˜ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìµœì‹  SOTA PLMì„ í™œìš©í•´ NLP ë° Text Miningì„ í•´ë³´ê¸°ë¥¼ 
ì›í•˜ëŠ” ë¶„ë“¤ì—ê²Œ ë„ì›€ì´ ë  ìˆ˜ ìˆê¸°ë¥¼ ë°”ëë‹ˆë‹¤. ê·¸ë¦¬ê³  ì•½ê°„ì˜ Domain Adaptationê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ë‹´ì•˜ìŠµë‹ˆë‹¤.

<br>

## Outline

- Introduction to Language Model for Financial domain
    - Language Model and Pretraining LM
    - ALBERT
    - Domain Adaptation to financial domain
    - transfer learning and some limitations
- ğŸ¤— Transformers
    - Features of Hugging Face's Transformers
    - Reasons why I use Transformers
- Fine-tuning ALBERT with Transformers 

### Korean notebooks

ì•„ì‰½ê²Œë„ KB-ALBERTë¥¼ ì™„ì „í•œ ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œí•˜ì§€ ëª»í•˜ì—¬, ë³„ë„ ì‹ ì²­ ê³¼ì •ì„ í†µí•´ì•¼ ëª¨ë¸ì„ ë‹¤ìš´ë°›ì•„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
[ê¹ƒí—™ ë§í¬](https://github.com/KB-Bank-AI/KB-ALBERT-KO/tree/master/kb-albert-char)ì—ì„œ ì‹ ì²­ë°©ë²•ì„ í™•ì¸í•´ì£¼ì„¸ìš”.

ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ BERT multilingual ëª¨ë¸ë¡œ ì•„ë˜ notebook ì˜ˆì œë“¤ì„ í…ŒìŠ¤íŠ¸í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
**ì‘ì„±ëœ ì˜ˆì œë“¤ì€ `bert-base-multilingual-cased`ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.**

| Notebooks | Description |  |
|:--- | :--- | ---: |
| [Features of ğŸ¤— Transformers](https://github.com/sackoh/pycon-korea-2020-kb-albert/blob/master/01-Features-of-Transformers.ipynb) | `Transformer`ì˜ ì£¼ìš” íŠ¹ì§•ë“¤ê³¼ ì˜ˆì œ ì½”ë“œ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sackoh/pycon-korea-2020-kb-albert/blob/master/01-Features-of-Transformers.ipynb) |
| [Fine-tuning ALBERT in PyTorch](https://github.com/sackoh/pycon-korea-2020-kb-albert/blob/master/02-Fine-tuning-ALBERT-in-PyTorch.ipynb) | PyTorchë¡œ ë„¤ì´ë²„ ì˜í™”ë¦¬ë·° ê°ì„±ë¶„ì„ì„ ìœ„í•œ ëª¨ë¸ì„ fine-tuning í•˜ëŠ” ì˜ˆì œ. GPU í™˜ê²½ ê°€ëŠ¥ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sackoh/pycon-korea-2020-kb-albert/blob/master/02-Fine-tuning-ALBERT-in-PyTorch.ipynb) |
| [Fine-tuning ALBERT in TensorFlow](https://github.com/sackoh/pycon-korea-2020-kb-albert/blob/master/03-Fine-tuning-ALBERT-in-TensorFlow.ipynb) | TensorFlowë¡œ ë„¤ì´ë²„ ì˜í™”ë¦¬ë·° ê°ì„±ë¶„ì„ì„ ìœ„í•œ ëª¨ë¸ì„ fine-tuning í•˜ëŠ” ì˜ˆì œ. GPU í™˜ê²½ ê°€ëŠ¥ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sackoh/pycon-korea-2020-kb-albert/blob/master/03-Fine-tuning-ALBERT-in-TensorFlow.ipynb) |
| [Fine-tuning ALBERT with TPU](https://github.com/sackoh/pycon-korea-2020-kb-albert/blob/master/04-Fine-tuning-ALBERT-with-TPU.ipynb) | TPUë¥¼ í†µí•´ í›¨ì”¬ ë¹ ë¥¸ ì†ë„ë¡œ ëª¨ë¸ì„ fine-tuning í•´ë³´ëŠ” ì˜ˆì œ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sackoh/pycon-korea-2020-kb-albert/blob/master/04-Fine-tuning-ALBERT-with-TPU.ipynb) |

<br>

### References
- https://github.com/huggingface/transformers
- https://github.com/KB-Bank-AI/KB-ALBERT-KO
